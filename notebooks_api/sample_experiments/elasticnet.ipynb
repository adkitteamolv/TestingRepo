{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93702522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample code for running experiments using mlflow.\n",
    "\"\"\"\n",
    "\n",
    "import cloudpickle,os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "\n",
    "\n",
    "import logging\n",
    "from refractio import get_local_dataframe\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    actual\n",
    "    pred\n",
    "    :returns\n",
    "    rmse, mae, r2\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def score_and_dump_func(file_path):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    file_path\n",
    "    \"\"\"\n",
    "    def score_func(model, request):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        model\n",
    "        request\n",
    "        :returns\n",
    "        score_output\n",
    "        \"\"\"\n",
    "        # Enter your custom score function here\n",
    "        \n",
    "        score_output=\"Success\"\n",
    "        return score_output\n",
    "    \n",
    "    with open(file_path, \"wb\") as out:\n",
    "        cloudpickle.dump(score_func, out)\n",
    "        \n",
    "    \n",
    "def experiment(alpha=0.5, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    alpha: 0.5 (default)\n",
    "    l1_ratio: 0.5 (default)\n",
    "    \"\"\"\n",
    "    # Tracking URI set\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URL\", \"http://mlflow-server\"))\n",
    "    \n",
    "    # Setting experiment name\n",
    "    mlflow.set_experiment(os.getenv(\"EXPERIMENT_NAME\", \"sample_experiment\"))\n",
    "\n",
    "    # Adding description to the experiment\n",
    "    tags = {'mlflow.note.content': os.getenv(\"EXPERIMENT_DESCRIPTION\", \"sample_description\")}\n",
    "\n",
    "    # Read the input csv file from /data mount attached to this notebook pod\n",
    "    input_csv_file = \"/data/\" + os.getenv(\"EXPERIMENT_DATASET\")\n",
    "    try:\n",
    "        data = get_local_dataframe(input_csv_file)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Unable to read training/test CSV.\\nError: {e}\")\n",
    "        \n",
    "    if not os.getenv(\"EXPERIMENT_TARGET_COLUMN\"):\n",
    "        logger.exception(\"Please provide target column!\")\n",
    "        raise Exception(\"Please provide target column!\")\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    # Registering datasets with mlflow experiment run\n",
    "    dataset = mlflow.data.from_pandas(data, source=input_csv_file)\n",
    "    train_dataset = mlflow.data.from_pandas(train)\n",
    "    test_dataset = mlflow.data.from_pandas(test)\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    train_x = train.drop([os.getenv(\"EXPERIMENT_TARGET_COLUMN\")], axis=1)\n",
    "    test_x = test.drop([os.getenv(\"EXPERIMENT_TARGET_COLUMN\")], axis=1)\n",
    "    train_y = train[[os.getenv(\"EXPERIMENT_TARGET_COLUMN\")]]\n",
    "    test_y = test[[os.getenv(\"EXPERIMENT_TARGET_COLUMN\")]]\n",
    "\n",
    "    with mlflow.start_run(tags=tags):\n",
    "        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        print(\"Elasticnet model (alpha={:f}, l1_ratio={:f}):\".format(alpha, l1_ratio))\n",
    "        print(\"  RMSE: %s\" % rmse)\n",
    "        print(\"  MAE: %s\" % mae)\n",
    "        print(\"  R2: %s\" % r2)\n",
    "\n",
    "        # logging params and metrics to the experiment runs\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "        # Log input data to MLflow run artifact.\n",
    "        mlflow.log_artifact(input_csv_file)\n",
    "\n",
    "        # Registering datasets with mlflow experiment run\n",
    "        mlflow.log_input(dataset, context=\"input\")\n",
    "        mlflow.log_input(train_dataset, context=\"train\")\n",
    "        mlflow.log_input(test_dataset, context=\"test\")\n",
    "\n",
    "        # Set custom tags\n",
    "        mlflow.set_tags({\n",
    "            \"template_id\": os.getenv(\"template_id\", \"sample_template_id\"),\n",
    "            \"notebook_name\": os.getenv(\"notebook_name\", \"sample_notebook_name\"),\n",
    "            \"algorithm\": \"ElasticNet\"       # Update this tag when trying with some other algos, this helps in filtering the runs based on algos.\n",
    "        })\n",
    "\n",
    "        predictions = lr.predict(train_x)\n",
    "        signature = infer_signature(train_x, predictions)\n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        # Storing score function for the model\n",
    "        score_and_dump_func(\"/tmp/scoring_func\")       \n",
    "        mlflow.log_artifact(\"/tmp/scoring_func\")\n",
    "        \n",
    "        # Model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "            # Register the model\n",
    "            # There are other ways to use the Model Registry, which depends on the use case,\n",
    "            # please refer to the doc for more information:\n",
    "            # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "            mlflow.sklearn.log_model(\n",
    "                lr, \"model\", registered_model_name=os.getenv(\"EXPERIMENT_NAME\", \"sample_experiment\"), signature=signature\n",
    "            )\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(lr, \"model\", signature=signature)\n",
    "        \n",
    "# Running Experiment with different alpha and l1_ratio params.\n",
    "experiment(alpha=0.5, l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b32ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
